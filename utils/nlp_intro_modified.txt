i. introduction
natural-language processing nlp is the intersection of computer science, linguistics and machine learning that is concerned with the communication between computers and humans in natural-language. nlp is all about enabling computers to understand and generate human language. applications of nlp techniques are voice assistants like alexa and siri but also things like machine translation and textfiltering. nlp is one of the fields that heavily benefited from the recent advances in machine learning, especially from deep-learning techniques. the field is divided into the three following parts
speech recognition  the translation of spoken language into text.
natural-language understanding  the computers ability to understand what we say.
natural-language generation  the generation of natural-language by a computer.
ii. why nlp is difficult
human language is special for several reasons. it is specifically constructed to convey the speakerwriters meaning. it is a complex system, although little children can learn it pretty quickly. another remarkable thing about human language is that it is all about symbols. according to chris manning machine learning professor at stanford university, it is a discrete, symbolic, categorical signaling system. this means that you can convey the same meaning by using different ways, like speech, gesture, signs etc. the encoding of these by the human brain is a continuous pattern of activation, where the symbols are transmitted via continuous signals of sound and vision.
understanding human language is considered a difficult task due to its complexity. for example, there is an infinite number of different ways to arrange words in a sentence. also, words can have several meanings and contextual information is necessary to correctly interpret sentences. every language is more or less unique and ambiguous. just take a look at the following newspaper headline the popes baby steps on gays. this sentence clearly has two very different interpretations, which is a pretty good example of the challenges in nlp.
note that a perfect understanding of language by a computer would result in an ai that can process the whole information that is available on the internet, which in turn would probably result in artificial general intelligence.
iii. syntactic & semantic analysis
syntactic analysis syntax and semantic analysis semantic are the two main techniques that lead to the understanding of natural-language. language is a set of valid sentences, but what makes a sentence valid? actually, you can break validity down into two things syntax and semantics. the term syntax refers to the grammatical structure of the text whereas the term semantics refers to the meaning that is conveyed by it. however, a sentence that is syntactically correct, does not have to be semantically correct. just take a look at the following example. the sentence cows flow supremely is grammatically valid subject  verb  adverb but does not make any sense.
syntactic analysis



httpspixabay.comdegrammatiklupelupenbuch0
syntactic analysis, also named syntax analysis or parsing is the process of analyzing natural-language conforming to the rules of a formal grammar. grammatical rules are applied to categories and groups of words, not individual words. syntactic analysis basically assigns a semantic structure to text.
for example, a sentence includes a subject and a predicate where the subject is a noun phrase and the predicate is a verb phrase. take a look at the following sentence the dog nounphrase went away verbphrase. note that we can combine every noun phrase with a verb phrase. like i already mentioned, sentences that are formed like that doesnt really have to make sense although they are syntactically correct.
semantic analysis
for us as humans, the way we understand what someone has said is an unconscious process that relies on our intuition and our knowledge about language itself. therefore, the way we understand language is heavily based on meaning and context. since computers can not rely on these techniques, they need a different approach. the word semantic is a linguistic term and means something related to meaning or logic.


httpspixabay.comdesinndownloadbegriffblickwinkel0
therefore, semantic analysis is the process of understanding the meaning and interpretation of words, signs, and sentence structure. this enables computers partly to understand natural-language the way humans do, involving meaning and context. i say partly because semantic analysis is one of the toughest parts of nlp and not fully solved yet. for example, speech recognition has become very good and works almost flawlessly but we are still lacking this kind of proficiency in natural-language understanding e.g semantic. your phone basically understands what you have said but often cant do anything with it because it doesnt understand the meaning behind it. also, note that some of the technologies out there only make you think they understand the meaning of a text. an approach based on keywords or statistics or even pure machine learning may be using a matching or frequency technique for clues as to what a text is about. these methods are limited because they are not looking at the real underlying meaning
iv. techniques to understand text
in the following; we will discuss many of the most popular techniques that are used for natural-language processing. note that some of them are closely intertwined and only serve as subtasks to solve larger problems.
parsing
what is parsing? lets, first of all, look into the dictionary
to parse
resolve a sentence into its component parts and describe their syntactic roles.
that actually nailed it but it could be a little bit more comprehensive. parsing refers to the formal analysis of a sentence by a computer into its constituents, which results in a parse tree that shows their syntactic relation to each other in visual form, which can be used for further processing and understanding.
below you can see a parse tree of the sentence the thief robbed the apartment, along with a description of the three different information types conveyed by it.



if we look at the letters directly above the single words, we can see that they show the part of speech of each word noun, verb, and determiner. if we look one level higher, we see some hierarchical grouping of words into phrases. for example, the thief is a noun phrase, robbed the apartment is a verb phrase and all together, they form a sentence, which is marked one level higher.
but what is actually meant by a nounor verbphrase? lets explain this with the example of noun phrase. these are phrases of one or more words that contain a noun and maybe descriptive words, verbs or adverbs. the idea is to group nouns with words that are in relation to them.
a parse tree also provides us with information about the grammatical relationships of the words due to the structure of their representation. for example, we can see in the structure that the thief is the subject of robbed.
with structure i mean that we have the verb robbed, which is marked with a v above it and a vp above that, which is linked with a s to the subject the thief, which has a np above. this is like a template for a subjectverb relationship and there are many others for other types of relationships.
stemming
stemming is a technique that comes from morphology and information retrieval which is used in nlp for preprocessing and efficiency purposes. but let us first look into the dictionary what stemming actually means
stem originate in or be caused by.
basically, stemming is the process of reducing words to their word stem but what is actually meant by stem? a stem is that part of a word that remains after the removal of all affixes. so for example, if you take a look at the word touched, its stem would be touch. touch is also the stem of touching and so on.
you may be asking yourself, why do we even need the stem? the stem is needed because you are going to encounter different variations of words that actually have the same stem and the same meaning. lets take a look at an example of two sentences
# i was taking a ride in the car
# i was riding in the car.
these two sentences mean the exact same thing and the use of the word is identical.
now, imagine all the english words in the vocabulary with all their different fixations at the end of them. to store them all would require a huge database that would contain many words that actually mean the same. this is solved by focusing only on a words stem, through stemming. popular algorithms are for example the porter stemming algorithm from 0, which works pretty good.
text segmentation
text segmentation in nlp is the process of transforming text into meaningful units which can be words, sentences, different topics, the underlying intent and much more. mostly, the text is segmented into its component words, which can be a difficult task, depending on the language. this is again due to the complexity of human language. for example, it works relatively well in english to separate words by spaces, except for words like ice box that belong together but are separated by a space. the problem is that people sometimes also write it as icebox.
named entity recognition
named entity recognition ner concentrates on determining which items in a text named entities can be located and classified into predefined categories. these categories can range from the names of persons, organization, locations to monetary values and percentages.
just take a look at the following example
before ner martin bought 0 shares of sap in 0.
after ner martinperson bought 0 shares of saporganization in 0time.
relationship extraction
relationship extraction takes the named entities of named entity recognition and tries to identify the semantic relationships between them. this could mean for example finding out who is married to whom, that a person works for a specific company and so on. this problem can also be transformed into a classification problem where you can train a machine learning model for every relationship type.
sentiment-analysis
with sentiment-analysis, we want to determine the attitude e.g the sentiment of, for example, a speaker or writer with respect to a document, interaction, or event. therefore it is a natural-language processing problem where text needs to be understood, to predict the underlying intent. the sentiment is mostly categorized into positive, negative and neutral categories. with the use of sentiment-analysis, we want to predict for example a customers opinion and attitude about a product based on a review he wrote about it. because of that, sentiment-analysis is widely applied to things like reviews, surveys, documents and much more.
if youre interested in using some of these techniques with python, you can take a look at the jupyter notebook about pythons natural-language toolkit nltk that i created. you can also check out my blog post about building neural networks with keras where i train a neural network to do sentiment-analysis.
v. deep-learning and nlp
now we know a lot about natural-language processing but the question that remains is, how do we use deep-learning in nlp.
central to deep-learning and natural-language is word meaning, where a word and especially its meaning are represented as a vector of real numbers. so with these vectors that represent words, we are placing words in a highdimensional space. the interesting thing about this is, that the words, which are represented by vectors, will act as a semantic space. this simply means that words that are similar and have a similar meaning tend to cluster together in this highdimensional vector space. you can see a visual representation of word meaning below

you can find out what a group of clustered words mean by doing principal component analysis pca or dimensionality reduction with tsne but this can be misleading sometimes because they oversimplify and leave a lot of information on the side. therefore, this is a good way to start like logistic or linear regression in data science but it isnt cutting edge and it is possible to do it way better.
we can also think of parts of words as vectors which represent their meaning. imagine the word undesirability. using a morphological approach, which involves the different parts a word has, we would think of it as being made out of morphemes wordparts like this un  desire  able  ity. every morpheme gets its own vector. this allows us to built a neural network out of that, which can compose the meaning of a larger unit, which in turn is made up of all of these morphemes.
deep-learning can also make sense of the structure of sentences, by creating syntactic parsers that can figure out the structure of sentences. google uses dependency parsing techniques like this, although in a more complex and larger manner, at their mcparseface and syntaxnet.
by knowing the structure of sentences, we can start trying to understand the meaning of sentences. like we already discussed, we start off with the meaning of words being vectors but we can also do this with whole phrases and sentences, where their meaning is also represented as vectors. and if we want to know the relationship of or between sentences, we train a neural network to make these decisions for us.
deep-learning works also good at sentiment-analysis. just look at the following moviereview this movie does not care about cleverness, with or any other kind of intelligent humor a traditional approach would have fallen into the trap of thinking this is a positive review, because cleverness or any other kind of intelligent humor sounds like a positive intent but a neural network would have recognized its real meaning. other applications are chatbots, machine translation, siri, google inboxes suggested replies and so on.
there also have been huge advancements in machine translation through the rise of recurrent neural networks, about which i also wrote a blogpost.
in machine translation done by deep-learning algorithms, language is translated by starting with a sentence and generating vector representations that represent it. then it starts to generate words in another language that entail the same information.
to summarize, nlp in combination with deep-learning is all about vectors that represent words, phrases etc. and also to some degree their meanings.
vi. summary
in this post, youve learned a lot about natural-language processing. now you know why nlp is such a difficult thing and why a perfect language understanding would probably result in artificial general intelligence. weve discussed the difference between syntactic and semantic analysis and learned about some nlp techniques that enable us to analyze and generate language. to summarize, the techniques weve discussed were parsing, stemming, text segmentation, named entity recognition, relationship extraction, and sentiment-analysis. on top of that, weve discussed how deep-learning managed to accelerate nlp by the concept of representing words, phrases, sentences and so on as numeric vectors.
