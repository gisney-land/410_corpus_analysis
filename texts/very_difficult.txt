






















Retrieval Methods: Feedback in Text Retrieval 

 
ChengXiang “Cheng” Zhai 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

1 



2 
Big Text Data 

Small Relevant Data 

  Search Engine 
Recommender  

System 

2. Text Access 

8. Recommendation 

3. Text Retrieval Problem 

7. Web Search 

User 

1. Natural Language Content Analysis 

4. Text Retrieval Methods 

5. Evaluation  

6. System  
Implementation 

4.3 Feedback in TR 

Text Retrieval Methods:  Feedback in TR 



3 

Relevance Feedback 

Updated 

query 

Feedback 

Judgments: 

d1 + 

d2 - 

d3 + 

… 

dk  - 

... 

Query Retrieval 
Engine 

Results: 

d1 3.5 

d2 2.4 

… 

dk  0.5 

... 
User 

Document 

collection 

Users make explicit relevance judgments on the initial results 

(judgments are reliable, but users don’t want to make extra effort) 



4 

Pseudo/Blind/Automatic Feedback 

Query Retrieval 
Engine 

Results: 

d1 3.5 

d2 2.4 

… 

dk  0.5 

... 

Judgments: 

d1 + 

d2 + 

d3 + 

… 

dk  - 

... 

Document 

collection 

Feedback 

Updated 

query 

 top 10  

assumed  

relevant 

Top-k initial results are simply assumed to be relevant 

(judgments aren’t reliable, but no user activity is required) 



5 

Implicit Feedback 

Updated 

query 

Feedback 

Clickthroughs: 

d1 + 

d2 - 

d3 + 

… 

dk  - 

... 

Query Retrieval 
Engine 

Results: 

d1 3.5 

d2 2.4 

… 

dk  0.5 

... 
User 

Document 

collection 

User-clicked docs are assumed to be relevant; skipped ones non-relevant  

(judgments aren’t completely reliable, but no extra effort from users) 



Feedback in Text Retrieval: Feedback in VSM  

 
ChengXiang “Cheng” Zhai 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

1 



2 

Feedback in Text Retrieval:  Feedback in VSM 



Feedback in Vector Space Model 

• How can a TR system learn from examples to improve 
retrieval accuracy?  

– Positive examples: docs known to be relevant 

– Negative examples: docs known to be non-relevant 

• General method: query modification 

– Adding new (weighted) terms (query expansion) 

– Adjusting weights of old terms 

3 



Rocchio Feedback: Illustration 

4 

+ 
q qm 

+ + 

+ + 
+ + 

+ 

+ + + 

+ 

+ 

+ 
+ 

+ 
- - - 

- 

- - - 

- 

- - - 

- 

- - - 
- - - - 

- 

- - - 
- 

- 

- 

- 
+ + + 

Centroid of  

non-relevant documents  Centroid of relevant documents  



5 

Rocchio Feedback: Formula 

Origial query 
Rel docs Non-rel docs 

Parameters 
New query 



Example of Rocchio Feedback 

6 

Query = “news about presidential campaign” 

… news about … D1 

… news about organic food campaign… D2 

… news of presidential campaign … D3 

… news of presidential campaign …  
… presidential candidate … 

D4 

… news of organic food campaign… campaign…campaign…campaign… D5 

V= {news about presidential camp. food …. } 

Q= (1, 1, 1, 1, 0, 0, …)   

- D1= (1.5, 0.1, 0, 0, 0, 0, …) 

- D2= (1.5, 0.1, 0, 2.0, 2.0, 0, …) 

+ D3= (1.5, 0, 3.0, 2.0, 0, 0, …)    

 + D4= (1.5, 0, 4.0, 2.0, 0, 0, …) 

- D5= (1.5, 0, 0, 6.0, 2.0, 0, …) 

+ Centroid Vector= ((1.5+1.5)/2, 0, (3.0+4.0)/2, (2.0+2.0)/2, 0, 0, …) 

=(1.5 , 0,  3.5, 2.0, 0, 0,…)    

- Centroid Vector= ((1.5+1.5+1.5)/3,   (0.1+0.1+0)/3, 0, (0+2.0+6.0)/3, (0+2.0+2.0)/3, 0, …) 

=(1.5 , 0.067, 0, 2.6, 1.3, 0,…)    

New Query Q’= (*1+*1.5-*1.5, *1-*0.067, *1+*3.5,  *1+*2.0-*2.6, -*1.3, 0, 0, …) 



7 

Rocchio in Practice 

• Negative (non-relevant) examples are not very important (why?) 

• Often truncate the vector  (i.e., consider only a small number of words that 
have highest weights in the centroid vector) (efficiency concern) 

• Avoid “over-fitting” (keep relatively high weight on the original query 
weights) (why?) 

• Can be used for relevance feedback and pseudo feedback ( should be set to 
a larger value for relevance feedback than for pseudo feedback) 

• Usually robust and effective 



Feedback in Text Retrieval: Feedback in LM 

 
ChengXiang “Cheng” Zhai 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

1 



2 
Big Text Data 

Small Relevant Data 

  Search Engine 
Recommender  

System 

2. Text Access 

8. Recommendation 

3. Text Retrieval Problem 

7. Web Search 

User 

1. Natural Language Content Analysis 

4. Text Retrieval Methods 

5. Evaluation  

6. System  
Implementation 

4.3 Feedback in TR 

Feedback in Text Retrieval:  Feedback in LM 



Feedback with Language Models 

• Query likelihood method can’t naturally support relevance 
feedback  

• Solution:  

– Kullback-Leibler (KL) divergence retrieval model as a generalization 
of query likelihood 

– Feedback is achieved through query model estimation/updating 

3 



d

qw
dw id

iSeen logn]
)C|w(p

)d|w(p
)[logq,w(c)d,q(f

i

i




 



4 

Kullback-Leibler (KL) Divergence Retrieval Model 








0)|w(p,dw

d

d

seen
Q

Q

log]
)C|w(p

)d|w(p
log)ˆ|w(p[)d,q(f

||

),(
)ˆ|(

Q

Qwc
wp Q 

Query Likelihood 

KL-divergence 
(cross entropy) 

Query LM 



Feedback as Model Interpolation 

5 

Query Q 

D

)||( DQD 

Document D 

Results 

Feedback Docs  

F={d1, d2 , …, dn} 
FQQ   )1('

Generative model 
 

Q

F
=0 

No feedback 

FQ  '

=1 

Full feedback 

QQ  '



6 

Generative Mixture Model 

w 

w 

F={d1, …, dn} 

log ( | ) ( ; ) log[(1 ) ( | ) ( | )]i
i w

p F c w d p w p w C      

)|(logmaxarg 


FpF Maximum Likelihood  

P(w|  ) 

P(w| C)  

1- 

P(source) 

Background words 

Topic words 

  = Noise in feedback documents 



 Mixture model 

approach 
 

Web database 
 

Top 10 docs 

Example of Pseudo-Feedback Query Model 

7 

W p(W|     )

security 0.0558

airport 0.0546

beverage 0.0488

alcohol 0.0474

bomb 0.0236

terrorist 0.0217

author 0.0206

license 0.0188

bond 0.0186

counter-terror 0.0173

terror 0.0142

newsnet 0.0129

attack 0.0124

operation 0.0121

headline 0.0121

 Query:  “airport security” 

W p(W|    )

the 0.0405

security 0.0377

airport 0.0342

beverage 0.0305

alcohol 0.0304

to 0.0268

of 0.0241

and 0.0214

author 0.0156

bomb 0.0150

terrorist 0.0137

in 0.0135

license 0.0127

state 0.0127

by 0.0125

=0.9 =0.7 
F

F



Summary of Feedback in Text Retrieval 

• Feedback = learn from examples  

• Three major feedback scenarios 
– Relevance, pseudo, and implicit feedback  

• Rocchio for VSM 

• Query model estimation for LM  
– Mixture model  

– Many other methods (e.g., relevance model)  have been proposed 
[Zhai 08] 

8 



Additional Readings 

• ChengXiang Zhai, Statistical Language Models for Information 
Retrieval (Synthesis Lectures Series on Human Language 
Technologies), Morgan & Claypool Publishers, 2008.  

http://www.morganclaypool.com/doi/abs/10.2200/S00158ED1V
01Y200811HLT001 

• Victor Lavrenko and W. Bruce Croft. 2001. Relevance based 
language models. In Proceedings of ACM SIGIR 2011.  

 

 
9 

http://www.morganclaypool.com/doi/abs/10.2200/S00158ED1V01Y200811HLT001
http://www.morganclaypool.com/doi/abs/10.2200/S00158ED1V01Y200811HLT001
http://www.morganclaypool.com/doi/abs/10.2200/S00158ED1V01Y200811HLT001


Web Search 

 
ChengXiang “Cheng” Zhai 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

1 



Web Search 

2 
Big Text Data 

Small Relevant Data 

  Search Engine 
Recommender  

System 

2. Text Access 

8. Recommendation 

3. Text Retrieval Problem 

7. Web Search 

User 

1. Natural Language Content Analysis 

4. Text Retrieval Methods 

6. System  
Implementation 

5. Evaluation 



Web Search: Challenges & Opportunities 

• Challenges 
– Scalability  

• How to handle the size of the Web and ensure completeness of 
coverage? 

• How to serve many user queries quickly?  

– Low quality information and spams 

– Dynamics of the Web 
• New pages are constantly created and some pages may be updated 

very quickly 

• Opportunities  
– many additional heuristics (e.g., links) can be leveraged to 

improve search accuracy  
3 

 Parallel indexing & searching (MapReduce) 

Spam detection  

    & Robust ranking 

Link analysis & multi-feature ranking 



4 

Basic Search Engine Technologies 

Cached 

pages 

Crawler 

Web 

---- 
---- 
… 
---- 
---- 

---- 
---- 
… 
---- 
---- 

… 
Indexer 

(Inverted) Index 

… 

Retriever 

Browser 

Query 

Host Info. 
Results 

User 



Component I: Crawler/Spider/Robot 

• Building a “toy crawler” is easy 
– Start with a set of “seed pages” in a priority queue 

– Fetch pages from the web 

– Parse  fetched pages for hyperlinks; add them to the queue 

– Follow the hyperlinks in the queue 

• A real crawler is much more complicated… 
– Robustness (server failure, trap, etc.) 

– Crawling courtesy (server load balance, robot exclusion, etc.) 

– Handling file types (images, PDF files, etc.) 

– URL extensions (cgi script, internal references, etc.) 

– Recognize redundant pages (identical and duplicates) 

– Discover “hidden” URLs (e.g., truncating a long URL ) 

5 



6 

Major Crawling Strategies 

• Breadth-First is common (balance server load) 

• Parallel crawling is natural  

• Variation: focused crawling  

– Targeting at a subset of pages (e.g., all pages about “automobiles” ) 

– Typically given a query 

• How to find new pages (they may not linked to an old page!) 

• Incremental/repeated crawling 

– Need to minimize resource overhead  

– Can learn from the past experience (updated daily vs. monthly)  

– Target at : 1) frequently updated pages; 2) frequently accessed pages  



Summary 

• Web search is one of the most important applications 
of text retrieval 

– New challenges: scalability, efficiency, quality of information 

– New opportunities: rich link information, layout, etc 

• Crawler is an essential component of Web search 
applications  

– Initial crawling: complete vs. focused 

– Incremental crawling: resource optimization  

7 



Web Search: Web Indexing 

 
ChengXiang “Cheng” Zhai 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

1 



Web Search: Web Indexing 

2 

Big Text Data 

Small Relevant Data 

  Search Engine 
Recommender  

System 

2. Text Access 

11. Recommendation 

3. Text Retrieval Problem 

10. Web Search 

User 

1. Natural Language Content Analysis 

4. Text Retrieval Methods 

7. Evaluation  

6. System  
Implementation 

5. Vector Space Model 

8. Probabilistic Model  

9. Feedback   



3 

Basic Search Engine Technologies 

Cached 

pages 

Crawler 

Web 

---- 
---- 
… 
---- 
---- 

---- 
---- 
… 
---- 
---- 

… Indexer 

(Inverted) Index 



4 

Overview of Web Indexing 

• Standard IR techniques are the basis, but insufficient  
– Scalability  
– Efficiency  

• Google’s contributions:   
– Google File System (GFS): distributed file system 
– MapReduce: Software framework for parallel computation 
– Hadoop: Open source implementation of MapReduce 



GFS Architecture 
Fixed chunk size (64 MB) 

Chunk is replicated 

to ensure reliability  

Simple centralized management  

Data transfer is directly 

 between application and  

chunk servers  

GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The google file system. In SOSP ’03: Proceedings of 
the nineteenth ACM symposium on Operating systems principles (New York, NY, USA, 2003), ACM, pp. 29–43. 

http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf 
 

http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf
http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf
http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf
http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf


MapReduce: A Framework for Parallel Programming  

• Minimize effort of programmer for simple parallel 
processing tasks 

• Features 
• Hide many low-level details (network, storage) 
• Built-in fault tolerance   
• Automatic load balancing  

6 



MapReduce: Computation Pipeline 

Map(K,V) 

Key, Value 

Key, Value 

Key, Value 

… 

Key, Value 

Key, Value 

… 

Key, Value 

Key, Value 

… 

Reduce(K, V[ ]) 

Key, Value Key, Value 

7 

Input   

Map(K,V) Map(K,V) 

Output  Key, Value 

Key, Value Key, Value 

MapReduce Internal Collection/Sorting 

Slide adapted from Alexander Behm & Ajey Shah’s presentation (http://www.slideshare.net/gothicane/behm-shah-pagerank) 



Word Counting 

Output:  

Count of each word  Input: Text Data 

How can we do this within the MapReduce framework? 
 

8 Slide adapted from Alexander Behm & Ajey Shah’s presentation (http://www.slideshare.net/gothicane/behm-shah-pagerank) 

Hello World Bye World 

Hello Hadoop Bye Hadoop 

Bye Hadoop Hello Hadoop 

…  … 

Bye 3 

Hadoop 4 

Hello 3 

World 2 

… 



Word Counting: Map Function 
Input 

 

1, “Hello World Bye World” 

 

 

 

 

2, “Hello Hadoop Bye Hadoop” 

 

…. 

 Map(K, V) 

 {  For each word w in V,   Collect(w, 1);} 

9 Slide adapted from Alexander Behm & Ajey Shah’s presentation (http://www.slideshare.net/gothicane/behm-shah-pagerank) 

Map(K,V) 
<Hello,1> 

<World,1> 

<Bye,1> 

<World,1> 

<Hello,1> 

<Hadoop,1> 

<Bye,1> 

<Hadoop,1> 

Output 

Map(K,V) 



Word Counting: Reduce Function 

Reduce(K, V[ ])  

{  Int count = 0;  For each v in V,  count += v;   Collect(K, count); } 

<Hello,1> 
<World,1> 
<Bye,1> 

<World,1> 
 

<Hello,1> 
<Hadoop,1> 

<Bye,1> 
<Hadoop,1> 

… 

 

<Bye  1, 1, 1> 
 

<Hadoop  1, 1, 1, 1> 
 

<Hello  1, 1, 1> 
 

 

<Bye, 3> 

 

<Hadoop, 4> 

 

<Hello, 3> 

10 Slide adapted from Alexander Behm & Ajey Shah’s presentation (http://www.slideshare.net/gothicane/behm-shah-pagerank) 

Reduce(K, V[ ]) 

Output 

Reduce(K, V[ ]) 

After  

internal grouping 

Reduce(K, V[ ]) 

Map Output 



11 

Inverted Indexing with MapReduce 

Built-In Shuffle and Sort: aggregate values by keys  

Map 

Reduce 

D1: java resource java class D2: java travel resource      D3: … 

Key               Value 

java               (D1, 2)   

resource       (D1, 1) 

class              (D1,1) 

Key               Value 

java               (D2, 1)   

travel            (D2,1) 

resource        (D2,1) 

Key               Value 

java               {(D1,2), (D2, 1)}   

resource        {(D1, 1), (D2,1)} 

class              {(D1,1)} 

travel            {(D2,1)} 

… Slide adapted from Jimmy Lin’s presentation 



Inverted Indexing: Pseudo-Code 

Slide adapted from Jimmy Lin’s presentation 12 



Summary 

• Web scale indexing requires  

– Storing the index on multiple machines (GFS) 

– Creating the index in parallel (MapReduce)  

• Both GFS and MapReduce are general infrastructures 

 

13 



Web Search: Link Analysis 

 
ChengXiang “Cheng” Zhai 

Department of Computer Science 

University of Illinois at Urbana-Champaign 

1 



Web Search: Link Analysis 

2 

Big Text Data 

Small Relevant Data 

  Search Engine 
Recommender  

System 

2. Text Access 

11. Recommendation 

3. Text Retrieval Problem 

10. Web Search 

User 

1. Natural Language Content Analysis 

4. Text Retrieval Methods 

7. Evaluation  

6. System  
Implementation 

5. Vector Space Model 

8. Probabilistic Model  

9. Feedback   



3 

Ranking Algorithms for Web Search 

• Standard IR models apply but aren’t sufficient 
– Different information needs  

– Documents have additional information 

– Information quality varies a lot 

• Major extensions 
– Exploiting links to improve scoring  

– Exploiting clickthroughs for massive implicit feedback  

– In general, rely on machine learning to combine all kinds of features 



4 

Exploiting Inter-Document Links 

Description 

(“anchor text”) 

Hub Authority 

“Extra text”/summary for a doc 

Links indicate the utility of a doc 

What does a link tell us?  



5 

PageRank: Capturing Page “Popularity” 
 

• Intuitions 
– Links are like citations in literature 
– A page that is cited often can be expected to be more useful in 

general 

• PageRank is essentially “citation counting”, but improves 
over simple counting 
– Consider “indirect citations” (being cited by a highly cited paper 

counts a lot…) 
– Smoothing of citations (every page is assumed to have a non-

zero pseudo citation count) 

• PageRank can also be interpreted as random surfing (thus 
capturing popularity) 























002/12/1

0010

0001

2/12/100

M

6 

The PageRank Algorithm 

Transition matrix 

Random surfing model: At any page,  

 With prob. , randomly jumping to another page 

 With prob. (1-), randomly picking a link to follow. 

p(di): PageRank score of di  =  average probability of visiting page di 
d1 

d2 

d4 

d3 

Mij = probability of going 

 from di to dj  

1
1




N

j

ijM

N= # pages 

“Equilibrium Equation”:  


 
N

i

itN

N

i

itijjt dpdpMdp
1

1

1

1 )()()1()( 

probability of visiting page dj at time t+1 probability of at page di at time t 

Reach dj via following a link 
Reach dj via random jumping 

Iij = 1/N 
pMIp T


))1((  



N

i

iijNj
dpMdp

1

1 )(])1([)( 
dropping the time index 

We can solve the equation with an iterative algorithm 



7 

PageRank: Example 
d1 

d2 

d4 

d3 

iterate until converge   Initial value p(d)=1/N,  

pMIp

dpMdp

T

N

i

iijNj


))1((

)(])1([)(
1

1














































































































































)(

)(

)(

)(

05.005.005.045.0

05.005.005.045.0

45.085.005.005.0

45.005.085.005.0

)(

)(

)(

)(

)(

)(

)(

)(

4/14/14/14/1

4/14/14/14/1

4/14/14/14/1

4/14/14/14/1

2.0

002/12/1

0010

0001

2/12/100

8.02.0)2.01(

4

3

2

1

4

3

2

1

4

1

3

1

2

1

1

1

dp

dp

dp

dp

dp

dp

dp

dp

A

dp

dp

dp

dp

IMA

n

n

n

n

n

n

n

n

T

n

n

n

n

)(*45.0)(*05.0)(*85.0)(*05.0)( 43211
1 dpdpdpdpdp nnnnn 

Do you see how scores are propagated over the graph?  



8 

PageRank in Practice 

• Computation can be quite efficient since M is usually 
sparse 

• Normalization doesn’t affect ranking, leading to some 
variants of the formula  

• The zero-outlink problem: p(di)’s don’t sum to 1 

–One possible solution = page-specific damping factor  
(=1.0 for a page with no outlink) 

• Many extensions (e.g., topic-specific PageRank) 

• Many other applications (e.g., social network analysis) 



9 

HITS: Capturing Authorities & Hubs 
 

• Intuitions 
– Pages that are widely cited are good authorities 

– Pages that cite many other pages are good hubs 

• The key idea  of HITS (Hypertext-Induced Topic Search) 
– Good authorities are cited by good hubs 

– Good hubs point to good authorities 

– Iterative reinforcement… 

• Many applications in graph/network analysis  

 



10 

The HITS Algorithm 

d1 

d2 

d4 
( )

( )

0 0 1 1

1 0 0 0

0 1 0 0

1 1 0 0

( ) ( )

( ) ( )

;

;

j i

j i

i j

d OUT d

i j

d IN d

T

T T

A

h d a d

a d h d

h Aa a A h

h AA h a A Aa





 
 
 
 
 
 





 

 





“Adjacency matrix” 

d3 Initial values: a(di)=h(di)=1 

Iterate 

Normalize:  
2 2

( ) ( ) 1i i
i i

a d h d  



Summary 

• Link information is very useful 

– Anchor text  

– PageRank 

– HITS 

• Both PageRank and HITS have many applications in 
analyzing other graphs or networks 

11 


	L1
	L2
	L3
	L4
	L5
	L6

